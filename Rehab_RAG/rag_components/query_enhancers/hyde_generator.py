class HydeQueryEnhancer:
    """
    [手法解説: HyDE (Hypothetical Document Embeddings)]
    ユーザーからの短い、あるいは曖昧な質問を、より検索に適した具体的な文章に変換するコンポーネントです。
    
    仕組み:
    1. ユーザーの質問を受け取る (例: 「脳卒中のリハビリは？」)
    2. LLMに「この質問に対する完璧な回答を想像して書いてください」とお願いする。
    3. LLMが架空の回答を生成する (例: 「脳卒中のリハビリテーションでは、急性期から...」)
    4. この生成された文章を、次の検索ステップの入力として使用する。

    期待される効果:
    - 短いクエリよりも多くの検索キーワードや文脈が含まれるため、検索精度が向上する。
    - ユーザーが思いつかない専門用語などをLLMが補ってくれる。
    """
    def __init__(self, llm):
        """
        コンストラクタ。文章生成のためのLLMインスタンスを受け取ります。
        
        Args:
            llm: GeminiLLMなど、`generate`メソッドを持つLLMラッパークラスのインスタンス。
        """
        self.llm = llm

    def enhance(self, query: str) -> str:
        """
        与えられたクエリから架空の理想的な回答を生成する (HyDE)。
        
        Args:
            query (str): ユーザーからの元の質問文。

        Returns:
            str: LLMによって生成された、検索用の架空の回答文。
        """
#         prompt = f"""以下の患者情報に基づいて、リハビリテーション総合実施計画書を作成するために**最も参考になるであろう、理想的な臨床ガイドラインやエビデンス**を要約したような文章を生成してください。
# この文章は、関連性の高い文献を検索するために使用します。個別の症例の詳細ではなく、**一般的な治療原則、評価のポイント、推奨される介入、予後**などに焦点を当ててください。

# # 患者情報
# {query}

# # 理想的な参考情報の要約:"""


        prompt = f"""以下の患者情報に関する質問に答えるために、臨床ガイドラインや質の高い研究論文から検索したいと考えています。
検索精度を高めるために、この質問に関連するであろう「架空のガイドライン抜粋」または「架空のエビデンスサマリー」を生成してください。
生成するテキストは、実際のガイドラインにあるような客観的で専門的な記述スタイル（推奨グレード、エビデンスレベル、具体的な治療法、リスク因子、予後予測などを含む）を模倣してください。

# 患者情報に関する質問 (検索の意図)
{query}

# 架空のガイドライン抜粋 または 架空のエビデンスサマリー (検索に使用するテキスト):"""


#         prompt = f"""以下の患者情報に基づいて、最適なリハビリテーション計画を作成するために参照すべき臨床ガイドラインやエビデンスを探しています。
# 検索精度を高めるため、この患者の **「診断名」「現在の主な問題点」「本人の希望・目標」** を **最重要視** し、関連するであろう「架空のガイドライン抜粋」または「架空のエビデンスサマリー」を生成してください。

# 生成するテキストは、以下の点を必ず含めてください:
# * 患者の **診断名** に特化した内容
# * 患者の **現在の主な問題点** （例：麻痺の程度、ADLの状況、合併症）に関連する具体的な推奨事項や注意点
# * 患者の **本人の希望・目標** （例：職場復帰、趣味の再開）を達成するためのアプローチ
# * 実際のガイドラインにあるような客観的で専門的な記述スタイル（推奨グレード、エビデンスレベル、具体的な治療法名など）

# # 患者情報 (検索の意図)
# {query}

# # 架空のガイドライン抜粋 または 架空のエビデンスサマリー (検索に使用するテキスト):"""
        
        hypothetical_answer = self.llm.generate(prompt, max_output_tokens=512)
        
        # LLMがエラーを返したり、空の文字列を生成した場合は、元のクエリをそのまま使う
        if "回答を生成できませんでした" in hypothetical_answer or not hypothetical_answer.strip():
            print("HyDEの生成に失敗したため、元のクエリを検索に使用します。")
            return query
            
        return hypothetical_answer